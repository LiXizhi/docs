# Paracraft基于延时渲染的多类型光源系统 v1.0

**软件用途：** Paracraft提供了基于延时渲染的多类型光源，可在世界内高效率使用大数量的光照渲染。
**运行环境:** Windows 10, Mac OS, Andriod, iOS
**编程语言：** NPL语言, C++
**开发完成时间：** 2018年6月1日
**发表日期：** 2018年6月1日
**技术特点：** 该产品在技术方面支持以下功能
- 支持大数量的光源渲染
- 支持点光源，平行光，聚光灯
- 所有光源参数实时可调节
- 效率高，占用资源少


**源代码行数**: 3万[点击这里查看](Paracraft基于延时渲染的多类型光源系统_code)

## 使用手册


In the latest work, we need the engine to support large number of lights.
As we all know, the classic way built in DirectX9 is not a reasonable method to do that because of
efficiency, so deferred shading way becomes the choice.

Adding new features in an existent system is never an easy thing. You'll encounter tons of ceilings
and pits.
After the feature finished I decided to build a simple project just for deferred shading method itself. That why this blog is here.


# Choice

Deferred shading has become the choice for our need. What is deferred shading and why we use it?
That's what this section is for.

## Rendering pipeline

The rendering pipeline provided by DirectX9 is not unfamiliar for most developers.

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4181/raw#image.png
  ext: png
  filename: image.png
  size: 33562
          
```


Lighting phase is ahead of Clipping phase.
Assuming the scene contains lots of objects, then the light work is heavy even the camera sees nothing.

In pseudo code, lighting work is done between each object and each light.

```
for object in all_objects:
    for light in all_lights:
        Lighting(object, light)
```

That's not an efficient way and it's impossible to place innumerable lights in a scene.

## Deferred pipeline

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4182/raw#image.png
  ext: png
  filename: image.png
  size: 28388
          
```


In deferred shading, there's a geometry stage to store all objects information in G-buffer in advance.
Then lighting stage reads data from G-buffer and lights pixels(not objects).

In pseudo code, those two stages are separated.

```
for object in all_objects:
    StoreInGBuffer(object)
for light in all_lights:
    Lighting(GBuffer, light)
```

The pixel number is determined by window size and resolution, and it's a constant.
So the complexity is lower than the old way.

## Conclusion

Efficiency is a main reason to choose deferred shading method,
but there is also weakness in deferred way.

The fatal one is that it doesn't support scene with transparency because a transparent pixel is composed by more than one point in different view depth.
But in G-buffer, we only store one point with the least depth.

# Architect

Let's talk about more details about deferred shading.

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4183/raw#image.png
  ext: png
  filename: image.png
  size: 28388
          
```



Clearly, there are 4 distinct stages in deferred shading: geometry stage, lighting stage, post-processing stage and final stage.
The first 2 stages are the main topic in this blog.

Each stage uses shaders and communicates with the other ones through a shared
memory area - the G-buffer.

## Geometry Stage

The geometry stage mainly draws all the objects in scene and makes perspective projection,
and feeds the G-buffer with related information to be used in the later stages.

Usually, G-buffer contains 4 texture areas to store the data for later use.
The textures have got the same size with the frame buffer.
- normal vector
- depth
- diffuse material information
- specular material information

Normal vector and depth are values in view space.
Diffuse material and specular material are the object properties.

## Lighting Stage

Lighting stage reads data from the G-buffer and does the lighting calculation.
In this stage, the G-buffer is as input for the lighting shaders.
It looks like in the lighting stage, we input 4 "images" and get 1 lighting "image" out.

A frequent question here is, all the data is stored in 2d screen space, how do we do
the lighting work?
I mean, lighting is a 3d world thing, related with distance, angles and something.

In the later section, we will see a method that recover the 3d space coordinate from the data
we have got.

## Post-Processing and Final Stage

This 2 stages are performed in order to enhance the image generated by lighting stage.
Those are long stories and not involved in this blog.

# Light Theory

Before we get into the implementation details, light theory is necessary for us.

In deferred shading, the lighting thing is on our own shoulder.
It's free to define any light effect as we like, but here we still stick on the traditional light model.

## Light Type

Commonly, there're [3 different kind of lights][light type]:
- directional light
- point light
- spot light

[light type]: https://docs.microsoft.com/en-us/windows/desktop/direct3d9/light-types

### Directional Light

Imaging the directional light as sun light is helpful.

Directional light has only color and direction, no position. It emits parallel light.

### Point Light


 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4184/raw#image.png'
  ext: png
  filename: image.png
  size: '2871'
  unit: '%'
  percent: 20

```



In the reality, the light bulb is a good example of point light.

Point light has color and position within a scene, but no single direction.
It gives off light equally in all directions.

### Spot Light


 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4185/raw#image.png'
  ext: png
  filename: image.png
  size: '12420'
  unit: '%'
  percent: 50

```


Thinking of spot light as a streetlight is a good idea.

Spot light has color, position and direction and it emits light.
Light emitted from a spot light is made up of a brighter inner
cone and a larger outer cone, with the light intensity diminishing between the two.

 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4186/raw#image.png'
  ext: png
  filename: image.png
  size: '13244'
  unit: '%'
  percent: 40

```


## Lighting Equation

It's about math and it's a tedious part but important.

### Symbols

First, let's denote the symbols we will use later.

$S$ means light $Source$, $I$ means $Illumination$, $M$ means $Material$, $C$ means $Camera$.

|symbol|definition|
|:--:|:--:|
|$S_{amb}$|ambient intensity color of light|
|$S_{diff}$|diffuse intensity color of light|
|$S_{spec}$|specular intensity color of light|
|$S_{pos}$|light position, only for point & spot light|
|$S_{dir}$|vector. light direction, only for directional & spot light|
|$S_{range}$|float number. denote the max lighting range, only for point & spot light|
|$S_{falloff}$|float number. denote the falloff parameter, only for spot light|
|$S_{att_0}$|float number. light attenuation parameter of distance$^0$|
|$S_{att_1}$|float number. light attenuation parameter of distance$^1$|
|$S_{att_2}$|float number. light attenuation parameter of distance$^2$|
|$M_{amb}$|ambient material color|
|$M_{diff}$|diffuse material color|
|$M_{spec}$|specular material color|
|$M_{shi}$|material specular shininess parameter|
|$C_{pos}$|camera position|

These symbols are defined for the simplicity of equations.

|symbol|definition|
|:--:|:--:|
|$p$|position of point to be shading|
|$\vec{n}$|unit vector. normal vector of point to be shading|
|$\vec{l}$|unit vector. denote the direction from point to light.<br>for directional light, $\vec{l} = -normalize(S_{dir})$. otherwise, $\vec{l} = normalize(S_{pos} - p)$|
|$\vec{v}$|unit vector. denote the direction from point to camera.<br>$\vec{v} = normalize(C_{pos} - p)$|
|$\vec{h}$|unit half vector between $\vec{l}$ and $\vec{v}$. used in [Phong shading model][phong model].<br>$\vec{h} = \dfrac{\vec{l} + \vec{v}}{length(\vec{l} + \vec{v})}$|

 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4187/raw#image.png'
  ext: png
  filename: image.png
  size: '34019'
  unit: '%'
  percent: 80

```



### I_tot

The total illumination equation we will use is:

$$
I_{tot} = I_{amb} + (att \times spot)(I_{diff} + I_{spec})
$$

|symbol|definition|
|:--:|:--:|
|$I_{tot}$|total illumination|
|$I_{amb}$|ambient illumination|
|$I_{diff}$|diffuse illumination|
|$I_{spec}$|specular illumination|
|$att$|light attenuation factor(related to the distance from point to light)|
|$spot$|spot light intensity factor(equals 1 for directional & point light)|

We will discuss each part in the subsections.

### I_amb

$$ I_{amb} = M_{diff} \otimes S_{amb} $$

Where $S_{amb}$ has the purpose of modulating the $M_{diff}$ value so that
all objects in scene reflect a small part of their diffuse contribution.

$\otimes$ means multiplication between two vectors in each element. for example,


 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4196/raw#image.png
  ext: png
  filename: image.png
  size: 26390
          
```


Ambient light is contributed to the whole scene whether the point is in or out the light range.
Theoretically, every time we add a light, the whole scene will be brighter no matter which kind of light is added.

### att

att is a parameter that control the light intensity influenced by distance.
In intuition, the closer, the brighter.

Directional light has got no position, so the att always equal 1.

For point and spot light, if a point is off the light range, it can't be lighted.
Within the range, light intensity is inversely proportional to the binomial of the distance.

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4197/raw#image.png
  ext: png
  filename: image.png
  size: 44029
          
```

### spot

[spot parameter][spot light factor] is only for spot light.
It's always 1 for directional and point light.

 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4188/raw#image.png'
  ext: png
  filename: image.png
  size: '12420'
  unit: '%'
  percent: 60

```



The affected area of spot light is a cone in 3d space.
This factor marks the light intensity in 3 different areas split by $\theta$ and $\phi$.

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4198/raw#image.png
  ext: png
  filename: image.png
  size: 57993
          
```


[spot light factor]: https://docs.microsoft.com/en-us/windows/desktop/direct3d9/attenuation-and-spotlight-factor

### I_diff

$$
I_{diff} = MAX(0, \vec{l} \cdot \vec{n}) M_{diff} \otimes S_{diff}
$$

This equation is based on [Lambert's Law][Lambert Law], the reflected light is determined by the cosine
between surface normal and light vector.

[Lambert Law]: https://www.wikiwand.com/en/Lambert%27s_cosine_law

### I_spec

$$
I_{spec} = MAX(0, \vec{h} \cdot \vec{n})^{M_{shi}} M_{spec} \otimes S_{spec}
$$

It's the famous [Phong reflection model][phong model].

[phong model]: https://www.wikiwand.com/en/Phong_reflection_model

# Implementation in DirectX9

This section focuses on the core parts in the [example project][project].

The implementation project is [here][project]. You'd better download it and
run it so you can see the whole picture and more details.


## Geometry Stage

### G-buffer

G-buffer contains 4 textures storing the data for all pipeline stages.
Using api `D3DXCreateTexture()` to create textures of `RENDERTARGET` type and holding their pointers.

```c++
IDirect3DTexture9* normalTex = 0;
IDirect3DSurface9* normalSurface = 0;

IDirect3DTexture9* depthTex = 0;
IDirect3DSurface9* depthSurface = 0;

IDirect3DTexture9* diffuseTex = 0;
IDirect3DSurface9* diffuseSurface = 0;

IDirect3DTexture9* specularTex = 0;
IDirect3DSurface9* specularSurface = 0;
```

```c++
bool SetupTexture(IDirect3DTexture9** texture, IDirect3DSurface9** surface)
{
	hr = D3DXCreateTexture(
		Device,
		Width,
		Height,
		D3DX_DEFAULT,
		D3DUSAGE_RENDERTARGET,
		D3DFMT_A8R8G8B8,
		D3DPOOL_DEFAULT,
		texture
	);

	if (FAILED(hr)) {
		::MessageBox(0, "Create texture error", 0, 0);
		return false;
	}

	(*texture)->GetSurfaceLevel(0, surface);

	return true;
}
```
```c++
/* prepare G-buffer textures */
if (!SetupTexture(&normalTex, &normalSurface)) {
	return false;
}
if (!SetupTexture(&depthTex, &depthSurface)) {
	return false;
}
if (!SetupTexture(&diffuseTex, &diffuseSurface)) {
	return false;
}
if (!SetupTexture(&specularTex, &specularSurface)) {
	return false;
}
if (!SetupTexture(&stashTex, &stashSurface)) {
	return false;
}
```



### Feed G-buffer

You'd better have got good understanding about shaders to read the content below.
If you don't, this book[^1] is good mentor for you.

DirectX9 supports [MRT(multiple render target)][MRT].
It refers to the ability to render to multiple surface with a single draw call.

[MRT]: https://docs.microsoft.com/en-us/windows/desktop/direct3d9/multiple-render-targets

Normally, we only set `COLOR0` value in pixel shader and that will be the final frame buffer color
showing on screen.

`0` in `COLOR0` means shader write the value to render target 0. It's frame buffer by default.
If we change the render target 0 or add more render targets, writing to `COLOR0` `COLOR1` means different things.

We have created 4 textures in G-buffer. In geometry stage we set this 4 textures as the render target.
So all the information will store in G-buffer.

```c++
void SetMRT()
{
	// save origin frame buffer render target and resume later
	Device->GetRenderTarget(0, &originRenderTarget);

	Device->SetRenderTarget(0, normalSurface);
	Device->SetRenderTarget(1, depthSurface);
	Device->SetRenderTarget(2, diffuseSurface);
	Device->SetRenderTarget(3, specularSurface);
}
```
```c++
struct PS_OUTPUT
{
    float4 normal : COLOR0;
    float4 depth : COLOR1;
    float4 diffuse : COLOR2;
    float4 specular : COLOR3;
};
```
```c++
PS_OUTPUT PS_Main(VS_OUTPUT input)
{
    PS_OUTPUT output = (PS_OUTPUT) 0;

    output.normal = float4(input.normal.xyz, 0);

    // zip depth in color range [0, 1]
    float depth = input.depth.x;
    float int_depth = floor(depth);
    float dec_depth = frac(depth);
    output.depth = float4((int_depth / 256.f) / 256.f, (int_depth % 256.f) / 256.f, dec_depth, 0);

    output.diffuse = float4(blue, 0);
    output.specular = float4(white, shininess / 256.f);

    return output;
}
```



## Lighting Stage

In this stage, we use 4 "images" to develop 1 final lighting "image".

### Read G-buffer

Object data is feed in G-buffer in geometry stage and we need to fetch data from G-buffer in lighting stage.

Inputting textures into shader is easy. It's textbook.

1. cancel the MRT binding
2. set textures as shader parameters
3. use `tex2D()` to fetch data from textures in shader


```c++
void ResumeOriginRender()
{
	Device->SetRenderTarget(0, originRenderTarget);

	Device->SetRenderTarget(1, NULL);
	Device->SetRenderTarget(2, NULL);
	Device->SetRenderTarget(3, NULL);
}
```
```c++
D3DXHANDLE normalHandle = effect->GetParameterByName(0, "normalTex");
D3DXHANDLE depthHandle = effect->GetParameterByName(0, "depthTex");
D3DXHANDLE diffuseHandle = effect->GetParameterByName(0, "diffuseTex");
D3DXHANDLE specularHandle = effect->GetParameterByName(0, "specularTex");
D3DXHANDLE stashHandle = effect->GetParameterByName(0, "stashTex");

effect->SetTexture(normalHandle, normalTex);
effect->SetTexture(depthHandle, depthTex);
effect->SetTexture(diffuseHandle, diffuseTex);
effect->SetTexture(specularHandle, specularTex);
effect->SetTexture(stashHandle, stashTex);
```
```c++
texture normalTex;
sampler normalSampler = sampler_state
{
    Texture = (normalTex);
    MinFilter = LINEAR;
    MagFilter = LINEAR;
    MipFilter = None;
    AddressU = clamp;
    AddressV = clamp;
};

texture depthTex;
sampler depthSampler = sampler_state
{
    Texture = (depthTex);
    MinFilter = LINEAR;
    MagFilter = LINEAR;
    MipFilter = None;
    AddressU = clamp;
    AddressV = clamp;
};

texture diffuseTex;
sampler diffuseSampler = sampler_state
{
    Texture = (diffuseTex);
    MinFilter = LINEAR;
    MagFilter = LINEAR;
    MipFilter = None;
    AddressU = clamp;
    AddressV = clamp;
};

texture specularTex;
sampler specularSampler = sampler_state
{
    Texture = (specularTex);
    MinFilter = LINEAR;
    MagFilter = LINEAR;
    MipFilter = None;
    AddressU = clamp;
    AddressV = clamp;
};
```



### Recover 3d coordinate

In rendering pipeline, we input 3d object vertex list and project them into 2d screen.
Recover 3d coordinate from 2d screen coordinate is a reverse process.

In this section, we only discuss recovering 3d coordinate in view space. It's simple and understandable.

Before we start, we must know how 3d coordinate in view space is projected.

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4190/raw#image.png
  ext: png
  filename: image.png
  size: 37788
          
```



In view space, camera coordinate is $(0, 0)$ and facing the $z$ axis.
Projection window coincides with plane $z = 1$. Its size is determined by $fov$
and $ratio$.
$fov$ is the camera view angle in y direction. $ratio$ is the ratio of screen width to screen height.


 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4191/raw#image.png'
  ext: png
  filename: image.png
  size: '4689'
  unit: '%'
  percent: 40

```


In [viewing frustum][viewing frustum], there're 2 clipping plane, front clipping plane and back clipping plane.
Usually, we set the front clipping plane to coincide with plane $z = 1$ too.

[viewing frustum]: https://docs.microsoft.com/en-us/windows/desktop/direct3d9/viewports-and-clipping

For a point $d$ in view frustum, we pick its $z$ value as the point depth after the projection.
And this $z$ value is stored in the G-buffer depth texture.

The point $d$ is projected to the point $d'$ in projection window.
Assuming the coordinate of $d$ is $(x, y, z)$, so the coordinate of $d'$ is $(x/z, y/z, 1)$.

If we have the 2d coordinate in projection window and depth, we know the 3d coordinate in 3d view space.

All those camera parameters are defined in api `D3DXMatrixPerspectiveFovLH()`.




```c++
D3DXMatrixPerspectiveFovLH(
	&proj,
	Fov,        // fov  angle
	ViewAspect, // view aspect
	1.0f,       // near plane
	1000.0f     // far  plane
);
```

### Draw viewport quad

So all things' ready, how do we start?
We have got G-buffer, light theory, recovering 3d coordinate method, how do we get the final lighting image?

We used to just throw all object vertex list into pipeline let it go.
So what is the vertex list for our final image?

Here is the answer, drawing a quad directly on the viewport skipping the local/world/camera/perspective transformation and get the image exactly equal to the viewport size.

Viewport coordinate system is arranged from $(-1, -1)$ to $(1, 1)$.

 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4192/raw#image.png'
  ext: png
  filename: image.png
  size: '16616'
  unit: '%'
  percent: 80

```



So we draw 2 triangles with vertexes $(-1, -1), (-1, 1), (1, -1), (1, 1)$ to form a quad.



```c++
/* prepare screen quad vertex buffer */
Device->CreateVertexBuffer(
	6 * sizeof(Vertex),
	0,
	D3DFVF_XYZ,
	D3DPOOL_MANAGED,
	&vb,
	0
);

/* screen quad coordinates

-1,1	        1,1
 v0             v1
  +-------------+
  |             |
  |    screen   |
  |             |
  +-------------+
 v2             v3
-1,-1          1,-1

*/
Vertex v0 = {
	-1, 1, 0
};
Vertex v1 = {
	1, 1, 0
};
Vertex v2 = {
	-1, -1, 0
};
Vertex v3 = {
	1, -1, 0
};

/* fill into buffer */
Vertex* vertices;
vb->Lock(0, 0, (void**)&vertices, 0);

vertices[0] = v0;
vertices[1] = v1;
vertices[2] = v2;
vertices[3] = v1;
vertices[4] = v3;
vertices[5] = v2;

vb->Unlock();
```
```c++
void DrawScreenQuad()
{
	Device->SetStreamSource(0, vb, 0, sizeof(Vertex));
	Device->SetFVF(D3DFVF_XYZ);
	Device->DrawPrimitive(D3DPT_TRIANGLELIST, 0, 2);
}
```



[Texture coordinate system][texture coordinate] is a little different from the viewport coordinate.
We need transform it before using `tex2D` fetch data from the texture.

[texture coordinate]: https://docs.microsoft.com/en-us/windows/desktop/direct3d9/texture-coordinates

 
```@BigFile
bigFile:
  src: 'https://api.keepwork.com/storage/v0/siteFiles/4193/raw#image.png'
  ext: png
  filename: image.png
  size: '21576'
  unit: '%'
  percent: 80

```


As to the projection window coordinate, we have seen it in the view frustum.
The coordinates are stretched 1-to-1 to viewport coordinates.

 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4194/raw#image.png
  ext: png
  filename: image.png
  size: 25799
          
```





```c++
VS_OUTPUT VS_Main(VS_INPUT input)
{
    VS_OUTPUT output;

    output.position = input.position;
    output.texCoord = input.position.xy * float2(0.5, -0.5) + float2(0.5, 0.5) + 0.5 / screenSize;
    output.cameraEye = float3(input.position.x * tanHalfFov * viewAspect, input.position.y * tanHalfFov, 1);

    return output;
};
```


What the `+ 0.5 / screenSize` is for? Check this [article][map tex to pixel] and you will see.

[map tex to pixel]: https://docs.microsoft.com/en-us/windows/desktop/direct3d9/directly-mapping-texels-to-pixels

### Lighting

Loop all the lights and accumulate each light intensity together.
Using the light equations above, it's math.

## Post-Processing and Final Stage

Not discussed in this example.

# Optimization

Deferred shading is an efficiency boost, yet there's room for optimization.

For each light, we have to loop on all the pixels to calculate the light intensity.
But usually point/spot light has limited range and can't light the whole scene.
If we skip the pixels not in light range, that's a big dent in calculation.

That's how the [stencil culling algorithm][stencil culling algorithm] works.

## Theory

Taking point light as example.

This time we draw the light range mesh as a object in scene instead of drawing the viewport quad.
We use 2 passes to distinguish the lighting area and no lighting area,
then do the lighting work in lighting area as usual.

Pass 1 creates a Stencil mask for the areas of the light volume that are not occluded by scene geometry.

Pass 1:
- Front (near) faces only
- Color write is disabled
- Z-write is disabled
- Z function is 'Less/Equal'
- Z-Fail writes value 1 to Stencil buffer (Stencil ref = 1)
- Stencil test result does not modify Stencil buffer

Pass 2 determines where lighting actually happens. Every pixel that passes Z and Stencil tests is then added to light pixel shader.

Pass 2:
- Back (far) faces only
- Color write enabled
- Z-write is disabled
- Z function is 'Greater/Equal'
- Stencil function is 'Equal' (Stencil ref = zero)
- Always clears Stencil to zero


 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4195/raw#image.png
  ext: png
  filename: image.png
  size: 136898
          
```

Light 1 has both near and far faces in front of all world geometry. It will pass Z test for near face, but will fail Z test for far face. Therefore pixel shader will not run.

Light 2 will fail Z test for near face, write non-zero value to Stencil and then fail Stencil test for far face. Again, no pixels will get through.

Light 3 will behave like Light 2 in the bottom part, but top part will pass both Z and Stencil tests and therefore will get rendered.

This method works well even the camera is in the light range. Brilliant!

## Implementation

Let's focus on the difference between optimization method and plain method in implementation.

drawing light range sphere mesh instead of drawing viewport quad.


```c++
		numPasses = 0;
		effect->Begin(&numPasses, 0);

		for (int i = 0; i < numPasses; i++)
		{
			effect->BeginPass(i);
#ifdef STENCIL_CULLING
			DrawSphere();
#else
			DrawScreenQuad();
#endif
			effect->EndPass();
		}
		effect->End();
```


And accordingly, the vertex shader get changes.

```c++
VS_OUTPUT VS_BackFace(VS_INPUT input)
{
    VS_OUTPUT output;

    float4 local_pos = input.position;
    float4x4 worldViewProj = mul(mul(world, view), proj);
    float4 proj_pos = mul(local_pos, worldViewProj);

    output.position = proj_pos;

    float4 norm_proj_pos = proj_pos / proj_pos.w;
    output.cameraEye = float3(norm_proj_pos.x * tanHalfFov * viewAspect, norm_proj_pos.y * tanHalfFov, 1);

    // + 0.5 / screenSize
    // https://docs.microsoft.com/en-us/windows/desktop/direct3d9/directly-mapping-texels-to-pixels
    output.texCoord = norm_proj_pos.xy * float2(0.5, -0.5) + float2(0.5, 0.5) + 0.5 / screenSize;

    return output;
}
```


Optimization has 2 passes just as the theory said. Plain method has 1 pass.
But they share the same lighting functions in pixel shader `PS_Main`.

```c++
Technique Plain
{
    Pass Light
    {
        VertexShader = compile vs_3_0 VS_Main();
        PixelShader = compile ps_3_0 PS_Main();
    }
}

Technique StencilCulling
{
    Pass FrontFace
    {
        VertexShader = compile vs_3_0 VS_FrontFace();
        PixelShader = compile ps_3_0 PS_FrontFace();

        ColorWriteEnable = 0;
        ZWriteEnable = 0;
        ZFunc = LESSEQUAL;

        StencilEnable = true;
        StencilFunc = ALWAYS;
        StencilZFail = REPLACE;
        StencilPass = KEEP;
        StencilRef = 1;
        StencilMask = 0xFFFFFFFF;

        CullMode = CCW;
    }
    Pass BackFace
    {
        VertexShader = compile vs_3_0 VS_BackFace();
        PixelShader = compile ps_3_0 PS_Main();

        ColorWriteEnable = 0xFFFFFFFF;
        ZWriteEnable = 0;
        ZFunc = GREATEREQUAL;

        StencilEnable = true;
        StencilFunc = EQUAL;
        StencilPass = ZERO;
        StencilRef = 0;
        StencilMask = 0xFFFFFFFF;

        CullMode = CW;
    }
}
```


## Paracraft light in use
 
 We use the light block to deploy different lights in the scene.
 All parameters of the light can be tuned freely.
 
 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4199/raw#image.png
  ext: png
  filename: image.png
  size: 488783
          
```
> 点光源


 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4200/raw#image.png
  ext: png
  filename: image.png
  size: 546962
          
```
> 聚光灯


 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4201/raw#image.png
  ext: png
  filename: image.png
  size: 630923
          
```
> 平行光


 Paracraft run in hight efficiency with lots of defered shading light sources.
 
```@BigFile

bigFile:
  src: >-
    https://api.keepwork.com/storage/v0/siteFiles/4203/raw#image.png
  ext: png
  filename: image.png
  size: 763444
          
```